<h1 align="center"> Introduction to Artificial Intelligence and Machine Learning</h1>
<br>
<p align='center'><img widht="900" src="https://cdn.business2community.com/wp-content/uploads/2018/01/what-is-machine-learning-dilbert.png"></p>
<br>
<br>
<h2> The Background Story and History of ML/AI:</h2>
<br>
<br>

### From Theory to Reality:

* **Pre 1940s:** Many of the major mathematical concepts of modern machine learning came from statistics. Few of those major breakthroughs
include **Bayes Thearem(1812)**, **Least Square Method(1805)**, **Markov Chain(1913)**. These techniques are all fundamental to
modern machine learning.
<br>
<br>



* **AI Debuts on the silver Screen(1927):** The sci-fi film **Metropolis** set in 2026 Berlin, introduced the audiences the idea
thinking machine. The character **False Maria** was the first robot ever depicted in a film.
<br>
<br>

<p align='center'><a href="https://www.youtube.com/watch?v=on2H8Qt5fgA" target="_blank">
<img style="border: 2px solid red" src="https://i1.wp.com/illuminatiwatcher.com/wp-content/uploads/2015/04/WO-Metropolis-False-Maria-Robot-Pentagram.jpg?w=450&ssl=1" 
alt="FALSE MARIA" width="600" height="400" border="10" /></a></p>

[YouTube Link](https://www.youtube.com/watch?v=on2H8Qt5fgA)

<br>
<br>


* **The Turing Test(1950):** Alan Turing an English Mathematician considered as the father of artificial intelligence,
created the **Turing Test** to determine if a computer has a real intelligence. To pass the test, a computer must be able to fool a human into believing
if it is also human.

<p align='center'><img width="200" src="https://upload.wikimedia.org/wikipedia/commons/a/a1/Alan_Turing_Aged_16.jpg"></p>

###### Turing Test
<p align='center'><img src="https://miro.medium.com/max/678/0*dGRyc5T7yqU6MRJH."></p>
<br>
<p align='center'><img src="https://i.pinimg.com/736x/47/14/eb/4714eb4d37d451c394b904197e40c4db.jpg"></p>
<!---
According to this kind of test, a computer is deemed to have artificial intelligence if it can mimic human responses under specific conditions.
In the basic Turing Test, there are three points. Two of the points are operated by humans, and the third point is operated by a computer. 
Each point is physically separated from the other two. One human is designated as the questioner. 
The other human and the computer are designated the respondents. The questioner interrogates both the human respondent and the computer according to a specified format, 
within a certain subject area and context, and for a preset length of time (such as 10 minutes). 
After the specified time, the questioner tries to decide which point is operated by the human respondent, 
and which one is operated by the computer. The test is repeated many times. 
If the questioner makes the correct determination in half of the test runs or less, the computer is considered to have artificial intelligence, 
because the questioner regards it as “just as human” as the human respondent.
-->

<br>
<br>

* **First Computer Learning Program(1952):** Machine learning pioneer Arthur Samuel created a program that helped an IBM computer get better at checkers the more it played.
Machine learning scientists often use board games because they are both understandable and complex.
<p align='center'><img width="500" src="https://lh3.googleusercontent.com/LwWR7kFeDM_sZmGKgehZXRZXGJz6iwxazEHpGxWWTlKmXfWwq78wobI6vTPiRo5DG1o=s1200"></p>
<br>
<br>

* **The Perceptron(1957):** Frank Rosenblatt – at the Cornell Aeronautical Laboratory – combined Donald Hebb’s model of brain cell interaction with Arthur Samuel’s Machine Learning efforts and created the perceptron. 
The perceptron was initially planned as a machine, not a program. 
The software, originally designed for the IBM 704, was installed in a custom-built machine called the Mark 1 perceptron, which had been constructed for image recognition.

<p align='center'><img width="400" src="https://miro.medium.com/max/392/0*GyDjJC-LfR2TNX6n."></p>

>Although the perceptron seemed promising, it could not recognize many kinds of visual patterns (such as faces), 
causing frustration and stalling neural network research. It would be several years before the frustrations of investors and funding agencies faded. 
Neural network/Machine Learning research struggled until a resurgence during the 1990s.
<p align='center'><img width="400" src="https://missinglink.ai/wp-content/uploads/2018/11/multilayer-perceptron.png"></p>

<!---
Machine Learning is, in part, based on a model of brain cell interaction. 
The model was created in 1949 by Donald Hebb in a book titled The Organization of Behavior (PDF). 
The book presents Hebb’s theories on neuron excitement and communication between neurons.
-->

<br>
<br>

* **The Nearest Neignbor Algorithm(1967):** The **Nearest Neighbor** algorithm was written, allowing computers to begin using very basic pattern recognition.
When the program was given a new object, it compared it with the existing data and classified it to the nearest neighbour, meaning the most similar object in memory.
This algorithm was used for mapping routes and was one of the earliest algorithms used in finding a solution to the **Traveling Salesperson’s** problem of finding the most efficient route.
Using it, a salesperson enters a selected city and repeatedly has the program visit the nearest cities until all have been visited.

<br>

<h4 align="center"> The Thinking Machine(Artificial Intelligence in the 1960s)</h4>
<p align='center'><a href="https://www.youtube.com/watch?time_continue=167&v=aygSMgK3BEM" target="_blank">
<img src="https://i.ibb.co/4VMHGsK/thinking-machine.png" width="400">
</a></p>

[YouTube Link](https://www.youtube.com/watch?time_continue=167&v=aygSMgK3BEM)

<br>
<br>

* **The years of 1970s and early 1980s:** In the late 1970s and early 1980s, neural network research was abandoned by computer science and AI researchers. 
The Machine Learning industry, which included a large number of researchers and technicians, was reorganized into a separate field and struggled for nearly a decade.
The industry goal shifted from training for Artificial Intelligence to solving practical problems in terms of providing services.
Its focus shifted from the approaches inherited from AI research to methods and tactics used in probability theory and statistics.
During this time, the ML industry maintained its focus on neural networks and then flourished in the 1990s.
Most of this success was a result of Internet growth, benefiting from the ever-growing availability of digital data and the ability to share its services by way of the Internet.

<br>
<br>

* **Back Propagation(1986):** Though **Back Propagation** was dreived by multiple researchers in the early 60s. However it was untill 1986,
with the publishing of a paper by Rumelhart, **Hinton**, and Williams, titled "Learning Representations by Back-Propagating Errors," that
the importance of the algorithm was appreciated by the machine learning community at large.  Yann LeCun, inventor of the Convolutional Neural Network architecture, 
proposed the modern form of the back-propagation learning algorithm for neural networks in his PhD thesis in 1987. But it is only much later, in 1993, 
that Wan was able to win an international pattern recognition contest through backpropagation
> By the 1980s, hand-engineering features had become the de facto standard in many fields, especially in computer vision,
since experts knew from experiments which features (e.g. lines, circles, edges, blobs in computer vision) made learning simpler.
However, hand-engineering successful features requires a lot of knowledge and practice. More importantly, since it is not automatic, it is usually very slow.
> Backpropagation was one of the first methods able to demonstrate that artificial neural networks could learn good internal representations, 
i.e. their hidden layers learned nontrivial features. Even more importantly, because of the efficiency of the algorithm and the fact that domain experts were no longer required to discover appropriate features, 
backpropagation allowed artificial neural networks to be applied to a much wider field of problems that were previously off-limits due to time and cost constraints.

<p align='center'><img src="https://pbs.twimg.com/media/D1odAE7XgAAHhYx.png"></p>


<br>
<br>

* **Machine Learning Applications(1990s):** Work on machine learning shifts from a knowledge-driven approach to a data-driven approach.  Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions — or “learn” — from the results. we began to apply machine learning in data mining, adaptive software and web applications, text learning, and language learning. Scientists begin creating programs for computers to analyze large amounts of data and draw conclusions — or “learn” — from the results.

<br>
<br>

* **Deep Blue Beats Garry Kasparov(1996):** Public awareness of AI increased greatly when an IBM computer named Deep Blue beat world chess champion Garry Kasparov in the first game of a match.
Kasparov won the 1996 match, but in 1997 an upgraded Deep Blue then won a second match 3½ games to 2½. Although Deep Blue played an impressive game of chess it largely relied on brute computing power to achieve this, including 480 special purpose ‘chess chips’. It worked by searching from 6-20 moves ahead at each position, having learned by evaluating thousands of old chess games to determine the path to checkmate.

<p align='center'><img src="https://ichef.bbci.co.uk/images/ic/496xn/p0549xh4.jpg"></p>

[YouTube Link](https://www.youtube.com/watch?v=KF6sLCeBj0s)

<br>
<br>

### Modern Machine Learning:

* **Neural Net research gets a reboot as "Deep Learning"(2006)**: Back in the early '80s, when Hinton and his colleagues first started work on this idea, computers weren’t fast or powerful enough to process the enormous collections of data that neural nets require. Their success was limited, and the AI community turned its back on them, working to find shortcuts to brain-like behavior rather than trying to mimic the operation of the brain.

But a few resolute researchers carried on. According to Hinton and LeCun, it was rough going. Even as late as 2004 – more than 20 years after Hinton and LeCun first developed the "back-propagation" algorithms that seeded their work on neural networks – the rest of the academic world was largely uninterested.

But that year, with a small amount of funding from the Canadian Institute for Advanced Research (CIFAR) and the backing of LeCun and Bengio, Hinton founded the Neural Computation and Adaptive Perception program, an invite-only group of computer scientists, biologists, electrical engineers, neuroscientists, physicists, and psychologists.

<!--
Hand-picking these researchers, Hinton aimed to create a team of world-class thinkers dedicated to creating computing systems that mimic organic intelligence – or at least what we know about organic intelligence, what we know about how the brain sifts through a wealth of visual, auditory, and written cues to understand and respond to its environment. Hinton believed creating such a group would spur innovation in AI and maybe even change the way the rest of world treated this kind of work.
-->

By then, they had the computing power they needed to realize many of their earlier ideas. As they came together for regular workshops, their research accelerated. They built more powerful deep learning algorithms that operated on much larger datasets. By the middle of the decade, they were winning global AI competitions. And by the beginning the current decade, the giants of the web began to notice.

<!--
<p align='center'><img src="https://images.thestar.com/wgf2KXhd5xhhKyMIEk-VAXShecA=/1200x799/smart/filters:cb(2700061000)/https://www.thestar.com/content/dam/thestar/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence/geoffrey-hinton-3.jpg" width="500"></p>
-->

[YouTube Link](https://youtu.be/uAu3jQWaN6E)
[For more info](https://www.wired.com/2014/01/geoffrey-hinton-deep-learning/)

<br>
<br>

* **CNN, LeNet5 and AlexNet and use of GPU in Machine Learning(2012):** It is the year 1994, and this is one of the very first convolutional neural networks, and what propelled the field of Deep Learning. This pioneering work by Yann LeCun was named LeNet5 after many previous successful iterations since the year 1988.

<p align='center'><img src="https://cdn-images-1.medium.com/max/800/0*V1vb9SDnsU1eZQUy.jpg"></p>

###### The Gap
In the years from 1998 to 2010 neural network were in incubation. Most people did not notice their increasing power, while many other researchers slowly progressed. More and more data was available because of the rise of cell-phone cameras and cheap digital cameras. And computing power was on the rise, CPUs were becoming faster, and GPUs became a general-purpose computing tool. Both of these trends made neural network progress, albeit at a slow rate. Both data and computing power made the tasks that neural networks tackled more and more interesting. And then it became clear…

###### The AlexNet
In 2012, Alex Krizhevsky released AlexNet which was a deeper and much wider version of the LeNet and won by a large margin the difficult ImageNet competition.
AlexNet is considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning.

<!--
AlexNet was not the first fast GPU-implementation of a CNN to win an image recognition contest. A CNN on GPU by K. Chellapilla et al. (2006) was 4 times faster than an equivalent implementation on CPU. A deep CNN of Dan Ciresan et al. (2011) at IDSIA was already 60 times faster and achieved superhuman performance in August 2011. Between May 15, 2011 and September 10, 2012, their CNN won no fewer than four image competitions. They also significantly improved on the best performance in the literature for multiple image databases.
According to the AlexNet paper, Ciresan's earlier net is "somewhat similar." Both were originally written with CUDA to run with GPU support. In fact, both are actually just variants of the CNN designs introduced by Yann LeCun et al. (1989) who applied the backpropagation algorithm to a variant of Kunihiko Fukushima's original CNN architecture called "neocognitron." The architecture was later modified by J. Weng's method called max-pooling
-->

<p align='center'><img src="https://cms.qz.com/wp-content/uploads/2018/06/2018-05-11-Alex-Krizhevsky-8394-e1529095310611.jpg?quality=75&strip=all&w=410&h=231"></p>

[Alex Krizhevsky](https://qz.com/1307091/the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley/)

### Advances in Machine Learning from then on

* **GoogleBrain (2012)**: This was a deep neural network created by Jeff Dean of Google, which focused on pattern detection in images and videos. It was able to use Google’s resources, which made it incomparable to much smaller neural networks. It was later used to detect objects in YouTube videos. [YouTube Link](https://www.youtube.com/watch?v=B15gWiOI64s)

<br>
<br>

* **DeepFace (2014):** This is a Deep Neural Network created by Facebook, which they claimed can recognise people with the same precision as a human can.

<br>
<br>

* **DeepMind (2014):** This company was bought by Google, and can play basic video games to the same levels as humans. In 2016, it managed to beat a professional at the game Go, which is considered to be one the world’s most difficult board games.
[YouTube Link](https://www.youtube.com/watch?v=jGyCsVhtW0M)

<br>
<br>

* **OpenAI (2015):** This is a non-profit organisation created by Elon Musk and others, to create safe artificial intelligence that can benefit humanity. OpenAI Five is the first AI to beat the world champions in
an esports game after defeating the reigning Dota 2 world champions, OG, at the OpenAI Five Finals on April 13, 2019.
OpenAI Five plays 180 years worth of games against itself every day, learning via self-play.

<p align='center'><img width="500" src="https://openai.com/content/images/2018/06/group-laptop.jpg"></p>

[YouTube Link](https://youtu.be/UZHTNBMAfAA)

<br>
<br>

* **Amazon Machine Learning Platform (2015):** This is part of Amazon Web Services, and shows how most big companies want to get involved in machine learning. They say it drives many of their internal systems, from regularly used services such as search recommendations and Alexa, to more experimental ones like Prime Air and Amazon Go.

<br>
<br>

* **YOLO - You Only Look Once: Unified, Real-Time Object Detection (2016):**
[Official Link](https://pjreddie.com/darknet/yolo/)

<br>
<br>

* **General Adverserial Network (2014):** Way back in 2014, Ian Goodfellow proposed a revolutionary idea — make two neural networks compete (or collaborate, it’s a matter of perspective) with each other.


One neural network tries to generate realistic data (note that GANs can be used to model any data distribution, but are mainly used for images these days), and the other network tries to discriminate between real data and data generated by the generator network.

Yann LeCun, Facebook’s chief AI scientist, has called GANs “the coolest idea in deep learning in the last 20 years.” 

<p align='center'><img width="500" src="https://miro.medium.com/max/1316/1*HaExieykcOT5oI2_xKisrQ.png"></p>

<p align='center'><img width="500" src="https://miro.medium.com/max/1682/1*UsiBSjHy8ut5GSAT8f7bIQ.png"></p>

<br>
<br>

<!--
The Importance of GPUs
Nvidia is behind one of the largest conferences on AI, and this is for a good reason - GPUs are extremely important in the world of machine learning. GPUs have around 200 times more processors per chip than CPUs. The flip side of this, however, is that whereas CPUs can perform any kind of computation, GPUs are tailored to only specific use cases, where operations (addition, multiplicaiton, etc.) have to be performed on vectors, which are essentially lists of numbers. A CPU would perform each operation on each number in the vector syncronously, i.e. one by one. This is slow. A GPU would perform operations on each number in the vector in parallel i.e. at the same time.

Vectors and matrices, which are grids of numbers (or lists of vectors) are essential to machine learning applications, and because of this, they are smaller, hence why more can be fit on one chip. Nvidia are credited with making the world’s first GPU, the GeForce 256 in 1999. At that time, launching the product was a risk as it was an entirely new kind of product. However, due to the use of vector calculations in video games, GPUs proliferated, as video games benefitted from a huge leap in performance. It was years later, than mathematicians, scientists and engineers realised that GPUs could be used to improve the speed of computations used in their discipline, due to the use of vectors. This led to the realisation that GPUs would make neural networks, a very old idea, leaps and bounds more practical. This led to GPU companies particularly Nvidia benefitting hugely from the “machine learning revolution”. Nvidia’s stock price has increased roughly 18-fold since 2012, the year in which the importance of GPUs in machine learning was demonstrated by AlexNet.

-->

#### 2018 Turing Award Winners
<br>

<p align='center'><img width="700" src="https://awards.acm.org/binaries/content/gallery/acm/ctas/awards/turing-2018-bengio-hinton-lecun.jpg"></p>
<br>

#### Honourable Mention
<br>
<p align='center'><img width="500" src="https://miro.medium.com/max/3200/1*FGGge_GilZ_KJYaoryaxkA.png"></p>
<br>
<p align='center'><img width="400" src="https://miro.medium.com/max/1080/1*GPas2MBwIQDzkDkDxMNI9w.jpeg"></p>
<h4 align="center"> Andrew Ng</h1>
<br>
<br>
<br>
<br>
<br>
<br>

## Misconceptions about ML/AI:
<br>
<p align='center'><img width="400" src="https://images-cdn.9gag.com/photo/aOYA1mE_700b.jpg"></p>
<br>
<p align='center'><img width="500" src="https://external-preview.redd.it/B_lQeKFyKYYGhEiMc3-FmczG4HxqnDMBtiWYNVcaExg.jpg?auto=webp&s=163d0092dacc73edd486174d80f770bca2c72410"></p>
<br>

##### Using millions of if-else statements in your code means you’re already aware of those if-else conditions and you’re writing a program to tackle the situation.

##### AI on the other hand in simple words means a program taking care of the other scenarios too which it is NOT programmed for. That means the program learns with new conditions around the environment and evolves accordingly.

<br>

### Venn Diagram of ML/AI:

<br>

<p align="center"><img width="500" src="https://whatsthebigdata.files.wordpress.com/2016/10/ai_data-science-diagram.jpg?w=640"></p>

<br>

<p align="center"><img width="500" src="https://s3.amazonaws.com/dev.assets.neo4j.com/wp-content/uploads/20181109071100/machine-learning-AI-2.png"></p>

<br>
<br>
<br>
<br>

## Exciting things happening in ML/AI around the world:



