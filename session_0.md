<h1 align="center"> Introduction to Artificial Intelligence and Machine Learning</h1>
<h2> The Background Story and History of ML/AI </h2>

#### The Timeline:

* **Pre 1940s:** Many of the major mathematical concepts of modern machine learning came from statistics. Few of those major breakthroughs
include **Bayes Thearem(1812)**, **Least Square Method(1805)**, **Markov Chain(1913)**. These techniques are all fundamental to
modern machine learning.
<br>
<br>



* **AI Debuts on the silver Screen(1927):** The sci-fi film **Metropolis** set in 2026 Berlin, introduced the audiences the idea
thinking machine. The character **False Maria** was the first robot ever depicted in a film.
<br>
<br>
<p align='center'><a href="https://www.youtube.com/watch?v=on2H8Qt5fgA" target="_blank">
<img style="border: 2px solid red" src="https://i1.wp.com/illuminatiwatcher.com/wp-content/uploads/2015/04/WO-Metropolis-False-Maria-Robot-Pentagram.jpg?w=450&ssl=1" 
alt="FALSE MARIA" width="600" height="400" border="10" /></a></p>
<br>
<br>


* **The Turing Test(1950):** Alan Turing an English Mathematician considered as the father of artificial intelligence,
created the **Turing Test** to determine if a computer has a real intelligence. To pass the test, a computer must be able to fool a human into believing
if it is also human.

<p align='center'><img width="200" src="https://upload.wikimedia.org/wikipedia/commons/a/a1/Alan_Turing_Aged_16.jpg"></p>

###### Turing Test
<p align='center'><img src="https://miro.medium.com/max/678/0*dGRyc5T7yqU6MRJH."></p>

<!---
According to this kind of test, a computer is deemed to have artificial intelligence if it can mimic human responses under specific conditions.
In the basic Turing Test, there are three points. Two of the points are operated by humans, and the third point is operated by a computer. 
Each point is physically separated from the other two. One human is designated as the questioner. 
The other human and the computer are designated the respondents. The questioner interrogates both the human respondent and the computer according to a specified format, 
within a certain subject area and context, and for a preset length of time (such as 10 minutes). 
After the specified time, the questioner tries to decide which point is operated by the human respondent, 
and which one is operated by the computer. The test is repeated many times. 
If the questioner makes the correct determination in half of the test runs or less, the computer is considered to have artificial intelligence, 
because the questioner regards it as “just as human” as the human respondent.
-->

<br>
<br>

* **First Computer Learning Program(1952):** Machine learning pioneer Arthur Samuel created a program that helped an IBM computer get better at checkers the more it played.
Machine learning scientists often use board games because they are both understandable and complex.
<p align='center'><img width="500" src="https://lh3.googleusercontent.com/LwWR7kFeDM_sZmGKgehZXRZXGJz6iwxazEHpGxWWTlKmXfWwq78wobI6vTPiRo5DG1o=s1200"></p>
<br>
<br>

* **The Perceptron(1957):** Frank Rosenblatt – at the Cornell Aeronautical Laboratory – combined Donald Hebb’s model of brain cell interaction with Arthur Samuel’s Machine Learning efforts and created the perceptron. 
The perceptron was initially planned as a machine, not a program. 
The software, originally designed for the IBM 704, was installed in a custom-built machine called the Mark 1 perceptron, which had been constructed for image recognition.

<p align='center'><img width="400" src="https://miro.medium.com/max/392/0*GyDjJC-LfR2TNX6n."></p>

>Although the perceptron seemed promising, it could not recognize many kinds of visual patterns (such as faces), 
causing frustration and stalling neural network research. It would be several years before the frustrations of investors and funding agencies faded. 
Neural network/Machine Learning research struggled until a resurgence during the 1990s.
<p align='center'><img width="400" src="https://missinglink.ai/wp-content/uploads/2018/11/multilayer-perceptron.png"></p>

<!---
Machine Learning is, in part, based on a model of brain cell interaction. 
The model was created in 1949 by Donald Hebb in a book titled The Organization of Behavior (PDF). 
The book presents Hebb’s theories on neuron excitement and communication between neurons.
-->

<br>
<br>

* **The Nearest Neignbor Algorithm(1967):** The **Nearest Neighbor** algorithm was written, allowing computers to begin using very basic pattern recognition.
When the program was given a new object, it compared it with the existing data and classified it to the nearest neighbour, meaning the most similar object in memory.
This algorithm was used for mapping routes and was one of the earliest algorithms used in finding a solution to the **Traveling Salesperson’s** problem of finding the most efficient route.
Using it, a salesperson enters a selected city and repeatedly has the program visit the nearest cities until all have been visited.

<br>
<br>

<h1 align="center"> The Thinking Machine(Artificial Intelligence in the 1960s)</h1>
<p align='center'><a href="https://www.youtube.com/watch?v=on2H8Qt5fgA" target="_blank">
<img style="border: 2px solid red" src="https://i1.wp.com/illuminatiwatcher.com/wp-content/uploads/2015/04/WO-Metropolis-False-Maria-Robot-Pentagram.jpg?w=450&ssl=1" 
alt="FALSE MARIA" width="600" height="400" border="10" /></a></p>

* **The years of 1970s and early 1980s:** In the late 1970s and early 1980s, neural network research was abandoned by computer science and AI researchers. 
The Machine Learning industry, which included a large number of researchers and technicians, was reorganized into a separate field and struggled for nearly a decade.
The industry goal shifted from training for Artificial Intelligence to solving practical problems in terms of providing services.
Its focus shifted from the approaches inherited from AI research to methods and tactics used in probability theory and statistics.
During this time, the ML industry maintained its focus on neural networks and then flourished in the 1990s.
Most of this success was a result of Internet growth, benefiting from the ever-growing availability of digital data and the ability to share its services by way of the Internet.

<br>
<br>

* **Back Propagation(1986):** Though **Back Propagation** was dreived by multiple researchers in the early 60s. However it was untill 1986,
with the publishing of a paper by Rumelhart, **Hinton**, and Williams, titled "Learning Representations by Back-Propagating Errors," that
the importance of the algorithm was appreciated by the machine learning community at large.  Yann LeCun, inventor of the Convolutional Neural Network architecture, 
proposed the modern form of the back-propagation learning algorithm for neural networks in his PhD thesis in 1987. But it is only much later, in 1993, 
that Wan was able to win an international pattern recognition contest through backpropagation
> By the 1980s, hand-engineering features had become the de facto standard in many fields, especially in computer vision,
since experts knew from experiments which features (e.g. lines, circles, edges, blobs in computer vision) made learning simpler.
However, hand-engineering successful features requires a lot of knowledge and practice. More importantly, since it is not automatic, it is usually very slow.
> Backpropagation was one of the first methods able to demonstrate that artificial neural networks could learn good internal representations, 
i.e. their hidden layers learned nontrivial features. Even more importantly, because of the efficiency of the algorithm and the fact that domain experts were no longer required to discover appropriate features, 
backpropagation allowed artificial neural networks to be applied to a much wider field of problems that were previously off-limits due to time and cost constraints.

<p align='center'><img src="https://pbs.twimg.com/media/D1odAE7XgAAHhYx.png"></p>


<br>
<br>

<iframe width="420" height="315"
src="https://www.youtube.com/embed/tgbNymZ7vqY">
</iframe>
